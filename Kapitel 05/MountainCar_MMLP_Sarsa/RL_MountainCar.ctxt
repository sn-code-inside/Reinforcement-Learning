#BlueJ class context
comment0.target=RL_MountainCar
comment0.text=\r\n\ MountainCar\ with\ Semigradient\ Sarsa\ and\ MMLPs.\r\n\ \r\n\ Supplementary\ material\ to\ the\ book\:\ \r\n\ "Reinforcement\ Learning\ From\ Scratch\:\ Understanding\ Current\ Approaches\ -\ with\ Examples\ in\ Java\ and\ Greenfoot"\ by\ Uwe\ Lorenz.\r\n\ https\://link.springer.com/book/10.1007/978-3-031-09030-1\r\n\ \r\n\ Ausgabe\ auf\ Deutsch\:\ https\://link.springer.com/book/9783662683101\r\n\ \r\n\ Licensing\ CC-BY-SA\ 4.0\ \r\n\ Attribution\ -\ Sharing\ under\ the\ same\ conditions\r\n\ \r\n\ www.facebook.com/ReinforcementLearningJava\r\n\ github.com/sn-code-inside/Reinforcement-Learning\r\n\r\n\ www.x-ai.eu\r\n\ \r\n\ @author\ Uwe\ Lorenz\r\n\ @version\ 1.2\ (14.11.2023)\r\n
comment1.params=
comment1.target=RL_MountainCar()
comment10.params=
comment10.target=void\ startNewEpisode()
comment10.text=\r\n\ A\ new\ episode\ is\ started.\ Update\ logging,\ reset\ counters\ and\ set\r\n\ agent\ to\ start\ position.\ Perform\ an\ evaluation\ period\ if\ necessary.\r\n
comment11.params=A_s\ s
comment11.target=java.lang.Integer\ getActionWithMaxQ(java.util.List,\ State)
comment11.text=\r\n\ Gets\ the\ action\ with\ the\ largest\ Q\ value\ for\ a\ given\ state.\ If\ there\ are\ several\ Q_max\ actions\r\n\ with\ the\ same\ value,\ they\ are\ selected\ randomly.\ If\ there\ are\ no\ Q-values,\ then\ -1\ is\ returned.\r\n\ @param\ s_key\ state\ key\r\n\ @return\ Action\ with\ greatest\ Q-value\ stored\ for\ the\ state\ s.\ Null\ if\ state\ is\ unknown.\r\n
comment12.params=
comment12.target=State\ getState()
comment13.params=
comment13.target=int\ getEpisode()
comment14.params=P
comment14.target=int\ selectAccordingToDistribution(double[])
comment14.text=\r\n\ Selection\ according\ to\ a\ given\ probability\ distribution\ P.\r\n\ @param\ P\ probability\ distribution\ over\ a\ discrete\ set\ of\ options\ (maybe\ actions).\r\n\ @return\ selected\ option,\ -1\ if\ no\ selection\ (error)\ \r\n
comment15.params=event
comment15.target=void\ handleLearningEvent(org.neuroph.core.events.LearningEvent)
comment16.params=
comment16.target=int[]\ getLayerSizesMLP()
comment17.params=
comment17.target=double\ getLearningRate()
comment2.params=world
comment2.target=void\ addedToWorld(greenfoot.World)
comment3.params=
comment3.target=void\ act()
comment4.params=
comment4.target=void\ sim_step()
comment5.params=s\ a\ reward\ s_new\ a_new\ episodeEnd
comment5.target=void\ update(State,\ int,\ double,\ State,\ int,\ boolean)
comment5.text=\r\n\ Sarsa\ update\ of\ the\ approximation\ of\ Q(s,a)\r\n\ @param\ s\ state\r\n\ @param\ a\ action\r\n\ @param\ reward\ Reward\r\n\ @param\ s_new\ Successor\ state\r\n\ @param\ a_new\ next\ action\r\n\ @param\ end\ Has\ a\ terminal\ state\ or\ the\ step\ limit\ been\ reached?\r\n
comment6.params=s\ a
comment6.target=double\ getQ(State,\ int)
comment7.params=s\ a\ observation
comment7.target=void\ setQ_toMLP(State,\ int,\ double)
comment7.text=\ \ private\ void\ setQ_MLP(Zustand\ z,\ int\ a,\ double\ beobachtung)\ \r\n\ \ \ \ {\ \r\n\ \ \ \ \ \ \ \ double[]\ input\ \=\ z.holeWerteArrayNormiert(minima,maxima);\ \r\n\ \ \ \ \ \ \ \ DataSet\ trainingSet\ \=\ new\ \ DataSet(DIM_input,\ DIM_output);\ \r\n\ \ \ \ \ \ \ \ double[]\ soll_output\ \=\ new\ double[DIM_output];\r\n\ \ \ \ \ \ \ \ soll_output[0]\=\ beobachtung;\r\n\ \ \ \ \ \ \ \ trainingSet.add(new\ DataSetRow(input,\ soll_output));\r\n\ \ \ \ \ \ \ \ neuralNetwork[a].learn(trainingSet);\r\n}
comment8.params=s\ a\ observation
comment8.target=void\ addQ_toMLP(State,\ int,\ double)
comment8.text=\r\n\ Adds\ a\ record\ for\ state-value\ function\ to\ batch.\r\n\ @param\ state\ features\ \r\n\ @param\ observation\ Observed\ evaluation\ of\ the\ state\r\n
comment9.params=n\ A_s\ s
comment9.target=double[]\ P_Policy(int,\ java.util.List,\ State)
comment9.text=\r\n\ Assigns\ a\ probability\ distribution\ to\ a\ state\ over\ the\ set\ of\ possible\ actions\r\n\ according\ to\ epsilon-greedy\ action\ selection\ strategy.\r\n\ @param\ n\ number\ of\ sucessor\ states\r\n\ @param\ A_s\ List\ of\ action\ options\ available\ to\ the\ agent\ at\ the\ given\ time\ in\ s.\r\n\ @param\ s\ state\r\n\ @return\ probability\ distribution\ for\ actions\ a\ in\ [0,1,...,n-1].\r\n
numComments=18
