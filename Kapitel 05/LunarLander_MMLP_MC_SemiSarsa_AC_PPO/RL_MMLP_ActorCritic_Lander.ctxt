#BlueJ class context
comment0.target=RL_MMLP_ActorCritic_Lander
comment0.text=\r\n\ Lunar-Lander\ with\ ActorCritic\ und\ MLPs.\r\n\ \r\n\ Supplementary\ material\ to\ the\ book\:\ \r\n\ "Reinforcement\ Learning\ From\ Scratch\:\ Understanding\ Current\ Approaches\ -\ with\ Examples\ in\ Java\ and\ Greenfoot"\ by\ Uwe\ Lorenz.\r\n\ https\://link.springer.com/book/10.1007/978-3-031-09030-1\r\n\ \r\n\ Ausgabe\ auf\ Deutsch\:\ https\://link.springer.com/book/9783662683101\r\n\ \r\n\ Licensing\ CC-BY-SA\ 4.0\ \r\n\ Attribution\ -\ Sharing\ under\ the\ same\ conditions\r\n\ \r\n\ www.facebook.com/ReinforcementLearningJava\r\n\ github.com/sn-code-inside/Reinforcement-Learning\r\n\r\n\ www.x-ai.eu\r\n\ \r\n\ @author\ Uwe\ Lorenz\r\n\ @version\ 1.2\ (14.11.2023)\r\n
comment1.params=
comment1.target=RL_MMLP_ActorCritic_Lander()
comment10.params=s\ observation
comment10.target=void\ setV_MLP(State,\ double)
comment10.text=\r\n\ Sets\ a\ value\ for\ state\ s\ to\ the\ estimator.\r\n\ @param\ s\ \r\n\ @param\ observation\ Observed\ evaluation\ of\ the\ state\r\n
comment11.params=x_s\ a
comment11.target=double\ get_h(double[],\ int)
comment11.text=\r\n\ Get\ action\ preference\ for\ given\ features.\r\n
comment12.params=x_s\ a\ h
comment12.target=void\ set_h_MLP(double[],\ int,\ double)
comment2.params=
comment2.target=void\ act()
comment3.params=
comment3.target=void\ startNewEpisode()
comment4.params=s\ a\ reward\ s_new\ episodeEnd
comment4.target=void\ update(State,\ int,\ double,\ State,\ boolean)
comment4.text=\r\n\ Actor-critic\ Update\r\n\ @param\ s\ state\r\n\ @param\ a\ action\r\n\ @param\ reward\ Reward\r\n\ @param\ s_new\ Successor\ state\r\n\ @param\ end\ Has\ a\ terminal\ state\ or\ the\ step\ limit\ been\ reached?\r\n
comment5.params=s
comment5.target=double[]\ P_Policy(State)
comment5.text=\r\n\ Stochastic\ policy\ of\ the\ agent.\ Assigns\ a\ probability\ distribution\ to\ a\ state\ over\ \r\n\ the\ set\ of\ possible\ actions.\r\n\ @param\ xs_key\ state\ key\r\n\ @return\ probability\ distribution\ for\ actions\ a\ in\ [0,1,...,n-1].\r\n
comment6.params=A_s\ s
comment6.target=double[]\ P_SoftMax(java.util.List,\ State)
comment6.text=\r\n\ Assigns\ a\ probability\ distribution\ to\ a\ feature\ vector\ over\ the\ set\ of\ possible\ actions\r\n\ according\ to\ softmax\ action\ selection\ strategy.\r\n\ @param\ n\ number\ of\ sucessor\ states\r\n\ @param\ A_s\ List\ of\ action\ options\ available\ to\ the\ agent\ at\ the\ given\ time\ in\ s.\r\n\ @param\ s_key\ key\ for\ given\ state\ s\r\n\ @return\ probability\ distribution\ for\ actions\ a\ in\ [0,1,...,n-1].\r\n
comment7.params=x_s\ a
comment7.target=double\ actionpreference(double[],\ int)
comment7.text=\r\n\ Calculates\ h(x(s,a),theta)\r\n\ @param\ x_sa\ state-action\ feature\ vector\r\n\ @return\ action\ preference\r\n
comment8.params=x_vec\ a
comment8.target=double[]\ get_x_sa(double[],\ int)
comment8.text=\r\n\ Creates\ the\ state-action\ feature\ vector.\r\n\ @param\ x_vec\ feature\ vector\ (from\ environment\ state)\r\n\ @param\ a\ action\r\n\ @return\ state-action\ feature\ vector\r\n
comment9.params=s
comment9.target=double\ getV(State)
comment9.text=\r\n\ Gets\ value\ for\ state\ s\ from\ the\ estimator.\r\n\ @param\ s\ state\ key\r\n\ @return\ value\ for\ state\ s.\r\n
numComments=13
