#BlueJ class context
comment0.target=RL_SemiGradientSarsa_Lander
comment0.text=\r\n\ Lunar-Lander\ with\ Semigradient\ Sarsa\ and\ MMLPs\ for\ approximating\ the\ value\ function.\r\n\r\n\ Supplementary\ material\ to\ the\ book\:\ \r\n\ "Reinforcement\ Learning\ From\ Scratch\:\ Understanding\ Current\ Approaches\ -\ with\ Examples\ in\ Java\ and\ Greenfoot"\ by\ Uwe\ Lorenz.\r\n\ https\://link.springer.com/book/10.1007/978-3-031-09030-1\r\n\ \r\n\ Ausgabe\ auf\ Deutsch\:\ https\://link.springer.com/book/9783662683101\r\n\ \r\n\ Licensing\ CC-BY-SA\ 4.0\ \r\n\ Attribution\ -\ Sharing\ under\ the\ same\ conditions\r\n\ \r\n\ www.facebook.com/ReinforcementLearningJava\r\n\ github.com/sn-code-inside/Reinforcement-Learning\r\n\r\n\ www.x-ai.eu\r\n\ \r\n\ @author\ Uwe\ Lorenz\r\n\ @version\ 1.2\ (14.11.2023)\r\n
comment1.params=
comment1.target=RL_SemiGradientSarsa_Lander()
comment10.params=n\ A_s\ s
comment10.target=double[]\ P_Policy(int,\ java.util.List,\ State)
comment10.text=\r\n\ Assigns\ a\ probability\ distribution\ to\ a\ state\ over\ the\ set\ of\ possible\ actions\r\n\ according\ to\ epsilon-greedy\ action\ selection\ strategy.\r\n\ @param\ n\ number\ of\ sucessor\ states\r\n\ @param\ A_s\ List\ of\ action\ options\ available\ to\ the\ agent\ at\ the\ given\ time\ in\ s.\r\n\ @param\ s\ state\r\n\ @return\ probability\ distribution\ for\ actions\ a\ in\ [0,1,...,n-1].\r\n
comment11.params=A_s\ s
comment11.target=java.lang.Integer\ getActionWithMaxQ(java.util.List,\ State)
comment11.text=\r\n\ Gets\ the\ action\ with\ the\ largest\ Q\ value\ for\ a\ given\ state.\ If\ there\ are\ several\ Q_max\ actions\r\n\ with\ the\ same\ value,\ they\ are\ selected\ randomly.\ If\ there\ are\ no\ Q-values,\ then\ -1\ is\ returned.\r\n\ @param\ s_key\ state\ key\r\n\ @return\ Action\ with\ greatest\ Q-value\ stored\ for\ the\ state\ s.\ Null\ if\ state\ is\ unknown.\r\n
comment12.params=s\ a\ reward\ s_new\ a_new\ episodeEnd
comment12.target=void\ update(State,\ int,\ double,\ State,\ int,\ boolean)
comment12.text=\r\n\ Update\ of\ the\ approximation\ of\ Q(s,a)\r\n\ @param\ s\ state\r\n\ @param\ a\ action\r\n\ @param\ reward\ Reward\r\n\ @param\ s_new\ Successor\ state\r\n\ @param\ a_new\ next\ action\r\n\ @param\ end\ Has\ a\ terminal\ state\ or\ the\ step\ limit\ been\ reached?\r\n
comment13.params=s\ a
comment13.target=double\ getQ(State,\ int)
comment13.text=\r\n\ Gets\ the\ estimate\ of\ the\ Q\ value\ from\ the\ MLP\ associated\ with\ a\ using\ s.\r\n\ @param\ s\ state\r\n\ @param\ a\ action\r\n\ @return\ approx.\ Q-value\r\n
comment14.params=s\ a\ observation
comment14.target=void\ setQ_toMLP(State,\ int,\ double)
comment14.text=\r\n\ Adjusts\ the\ MLP\ belonging\ to\ a.\r\n\ @param\ s\ state\r\n\ @param\ a\ action\r\n\ @param\ observation\ target\ value\r\n
comment15.params=event
comment15.target=void\ handleLearningEvent(org.neuroph.core.events.LearningEvent)
comment16.params=s
comment16.target=double\ maxQ(State)
comment17.params=
comment17.target=State\ getState()
comment18.params=
comment18.target=int\ episode()
comment19.params=
comment19.target=double\ getReward()
comment2.params=input_dim\ output_dim
comment2.target=void\ initTrainigSets(int,\ int)
comment20.params=
comment20.target=void\ sim_step()
comment21.params=
comment21.target=double\ checkReward()
comment21.text=\r\n\ Calculates\ the\ reward\ from\ the\ agent's\ state.\r\n\ @return\ reward\r\n
comment22.params=
comment22.target=int[]\ getLayerSizesMLP()
comment23.params=
comment23.target=double\ getLearningRate()
comment24.params=P
comment24.target=int\ selectAccordingToDistribution(double[])
comment24.text=\r\n\ Selection\ according\ to\ a\ given\ probability\ distribution\ P.\r\n\ @param\ P\ probability\ distribution\ over\ a\ discrete\ set\ of\ options\ (maybe\ actions).\r\n\ @return\ selected\ option,\ -1\ if\ no\ selection\ (error)\ \r\n
comment3.params=world
comment3.target=void\ addedToWorld(greenfoot.World)
comment3.text=\r\n\ Lander\ has\ been\ added\ to\ the\ world.\r\n
comment4.params=
comment4.target=void\ act()
comment5.params=action
comment5.target=void\ setAction(int)
comment6.params=
comment6.target=void\ startNewEpisode()
comment7.params=
comment7.target=void\ adjustMLPs()
comment7.text=\ \r\n\ Adjusts\ the\ MLPs\ with\ the\ recorded\ training\ data.\r\n
comment8.params=
comment8.target=void\ trainingDataReset()
comment9.params=
comment9.target=void\ landerOnStart()
numComments=25
